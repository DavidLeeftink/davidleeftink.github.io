---
layout: about
title: about
permalink: /
subtitle: #<a href='#'>Machine Learning · Control Theory · Reinforcement Learning</a>. 

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Radboud University</p>
    <p>Nijmegen, the Netherlands </p>
    <p> david.leeftink (at) ru.nl. </p>

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: true
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---
I'm a PhD candidate at the Donders Institute for Brain and Cognition at Radboud University, working closely with [Dr. Max Hinne](https://www.ru.nl/personen/hinne-m) and [Prof. Marcel van Gerven](https://www.ru.nl/personen/gerven-m-van).

I am particularly interested in the intersection of <span style="color: var(--global-theme-color)"> (probabilistic) machine learning</span>, <span style="color: var(--global-theme-color)"> dynamical systems</span>, and <span style="color: var(--global-theme-color)"> control theory</span>; this work frames my approach to <span style="color: var(--global-theme-color)">reinforcement learning</span>. A central question throughout my work is: how do we make **trustworthy** and **robust** decisions under model **uncertainty**?

On the *practical* side, I have developed and contributed to decision-making algorithms that have been successfully deployed in real-world settings, including optimizing [semiconductor manufacturing processes](https://arxiv.org/abs/2511.23141), and adapting stimulation patterns of [neural implants](https://iopscience.iop.org/article/10.1088/1741-2552/adeae9/meta) in the human brain to evoke visual percepts in a blind patient.

On the *theoretical* side, I study how ideas from optimal control theory can serve as a foundation for analyzing deep reinforcement learning. While modern methods achieve groundbreaking results, they are often notorious for their fragility and sample inefficiency. My aim is to root these methods in the notions of <span style="color: var(--global-theme-color)">stability</span>, <span style="color: var(--global-theme-color)">robustness</span>, and <span style="color: var(--global-theme-color)">safety</span>; characteristics that are **essential** for intelligent decision-making in **high-stakes** applications such as healthcare, energy, and robotics.
